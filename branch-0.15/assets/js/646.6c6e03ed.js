(window.webpackJsonp=window.webpackJsonp||[]).push([[646],{1035:function(e,_,s){"use strict";s.r(_);var t=s(43),a=Object(t.a)({},(function(){var e=this,_=e.$createElement,s=e._self._c||_;return s("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[s("h1",{attrs:{id:"导出查询结果集"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#导出查询结果集"}},[e._v("#")]),e._v(" 导出查询结果集")]),e._v(" "),s("p",[e._v("本文档介绍如何使用 "),s("code",[e._v("SELECT INTO OUTFILE")]),e._v(" 命令进行查询结果的导出操作。")]),e._v(" "),s("h2",{attrs:{id:"语法"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#语法"}},[e._v("#")]),e._v(" 语法")]),e._v(" "),s("p",[s("code",[e._v("SELECT INTO OUTFILE")]),e._v(" 语句可以将查询结果导出到文件中。目前支持通过 Broker 进程, 通过 S3 协议, 或直接通过 HDFS 协议，导出到远端存储，如 HDFS，S3，BOS，COS（腾讯云）上。语法如下")]),e._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[e._v('query_stmt\nINTO OUTFILE "file_path"\n[format_as]\n[properties]\n')])])]),s("ul",[s("li",[s("p",[s("code",[e._v("file_path")])]),e._v(" "),s("p",[s("code",[e._v("file_path")]),e._v(" 指向文件存储的路径以及文件前缀。如 "),s("code",[e._v("hdfs://path/to/my_file_")]),e._v("。")]),e._v(" "),s("p",[e._v("最终的文件名将由 "),s("code",[e._v("my_file_")]),e._v("，文件序号以及文件格式后缀组成。其中文件序号由0开始，数量为文件被分割的数量。如：")]),e._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[e._v("my_file_abcdefg_0.csv\nmy_file_abcdefg_1.csv\nmy_file_abcdegf_2.csv\n")])])])]),e._v(" "),s("li",[s("p",[s("code",[e._v("[format_as]")])]),e._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[e._v("FORMAT AS CSV\n")])])]),s("p",[e._v("指定导出格式。默认为 CSV。")])]),e._v(" "),s("li",[s("p",[s("code",[e._v("[properties]")])]),e._v(" "),s("p",[e._v("指定相关属性。目前支持通过 Broker 进程, 或通过 S3 协议进行导出。")]),e._v(" "),s("ul",[s("li",[e._v("Broker 相关属性需加前缀 "),s("code",[e._v("broker.")]),e._v("。具体参阅"),s("RouterLink",{attrs:{to:"/zh-CN/administrator-guide/broker.html"}},[e._v("Broker 文档")]),e._v("。")],1),e._v(" "),s("li",[e._v("HDFS 相关属性需加前缀 "),s("code",[e._v("hdfs.")]),e._v("。")]),e._v(" "),s("li",[e._v("S3 协议则直接执行 S3 协议配置即可。")])]),e._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[e._v('("broker.prop_key" = "broker.prop_val", ...)\nor\n("hdfs.fs.defaultFS" = "xxx", "hdfs.hdfs_user" = "xxx")\nor \n("AWS_ENDPOINT" = "xxx", ...)\n')])])]),s("p",[e._v("其他属性：")]),e._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[e._v('("key1" = "val1", "key2" = "val2", ...)\n')])])]),s("p",[e._v("目前支持以下属性：")]),e._v(" "),s("ul",[s("li",[s("code",[e._v("column_separator")]),e._v("：列分隔符，仅对 CSV 格式适用。默认为 "),s("code",[e._v("\\t")]),e._v("。")]),e._v(" "),s("li",[s("code",[e._v("line_delimiter")]),e._v("：行分隔符，仅对 CSV 格式适用。默认为 "),s("code",[e._v("\\n")]),e._v("。")]),e._v(" "),s("li",[s("code",[e._v("max_file_size")]),e._v("：单个文件的最大大小。默认为 1GB。取值范围在 5MB 到 2GB 之间。超过这个大小的文件将会被切分。")]),e._v(" "),s("li",[s("code",[e._v("schema")]),e._v("：PARQUET 文件schema信息。仅对 PARQUET 格式适用。导出文件格式为PARQUET时，必须指定"),s("code",[e._v("schema")]),e._v("。")])])])]),e._v(" "),s("h2",{attrs:{id:"并发导出"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#并发导出"}},[e._v("#")]),e._v(" 并发导出")]),e._v(" "),s("p",[e._v("默认情况下，查询结果集的导出是非并发的，也就是单点导出。如果用户希望查询结果集可以并发导出，需要满足以下条件：")]),e._v(" "),s("ol",[s("li",[e._v("session variable 'enable_parallel_outfile' 开启并发导出: "),s("code",[e._v("set enable_parallel_outfile = true;")])]),e._v(" "),s("li",[e._v("导出方式为 S3 , 或者 HDFS， 而不是使用 broker")]),e._v(" "),s("li",[e._v("查询可以满足并发导出的需求，比如顶层不包含 sort 等单点节点。（后面会举例说明，哪种属于不可并发导出结果集的查询）")])]),e._v(" "),s("p",[e._v("满足以上三个条件，就能触发并发导出查询结果集了。并发度 = "),s("code",[e._v("be_instacne_num * parallel_fragment_exec_instance_num")])]),e._v(" "),s("h3",{attrs:{id:"如何验证结果集被并发导出"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#如何验证结果集被并发导出"}},[e._v("#")]),e._v(" 如何验证结果集被并发导出")]),e._v(" "),s("p",[e._v("用户通过 session 变量设置开启并发导出后，如果想验证当前查询是否能进行并发导出，则可以通过下面这个方法。")]),e._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[e._v('explain select xxx from xxx where xxx  into outfile "s3://xxx" format as csv properties ("AWS_ENDPOINT" = "xxx", ...);\n')])])]),s("p",[e._v("对查询进行 explain 后，Doris 会返回该查询的规划，如果你发现 "),s("code",[e._v("RESULT FILE SINK")]),e._v(" 出现在 "),s("code",[e._v("PLAN FRAGMENT 1")]),e._v(" 中，就说明导出并发开启成功了。\n如果 "),s("code",[e._v("RESULT FILE SINK")]),e._v(" 出现在 "),s("code",[e._v("PLAN FRAGMENT 0")]),e._v(" 中，则说明当前查询不能进行并发导出 (当前查询不同时满足并发导出的三个条件)。")]),e._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[e._v("并发导出的规划示例：\n+-----------------------------------------------------------------------------+\n| Explain String                                                              |\n+-----------------------------------------------------------------------------+\n| PLAN FRAGMENT 0                                                             |\n|  OUTPUT EXPRS:<slot 2> | <slot 3> | <slot 4> | <slot 5>                     |\n|   PARTITION: UNPARTITIONED                                                  |\n|                                                                             |\n|   RESULT SINK                                                               |\n|                                                                             |\n|   1:EXCHANGE                                                                |\n|                                                                             |\n| PLAN FRAGMENT 1                                                             |\n|  OUTPUT EXPRS:`k1` + `k2`                                                   |\n|   PARTITION: HASH_PARTITIONED: `default_cluster:test`.`multi_tablet`.`k1`   |\n|                                                                             |\n|   RESULT FILE SINK                                                          |\n|   FILE PATH: s3://ml-bd-repo/bpit_test/outfile_1951_                        |\n|   STORAGE TYPE: S3                                                          |\n|                                                                             |\n|   0:OlapScanNode                                                            |\n|      TABLE: multi_tablet                                                    |\n+-----------------------------------------------------------------------------+\n")])])]),s("h2",{attrs:{id:"使用示例"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#使用示例"}},[e._v("#")]),e._v(" 使用示例")]),e._v(" "),s("ol",[s("li",[s("p",[e._v("示例1")]),e._v(" "),s("p",[e._v("使用 broker 方式导出，将简单查询结果导出到文件 "),s("code",[e._v("hdfs:/path/to/result.txt")]),e._v("。指定导出格式为 CSV。使用 "),s("code",[e._v("my_broker")]),e._v(" 并设置 kerberos 认证信息。指定列分隔符为 "),s("code",[e._v(",")]),e._v("，行分隔符为 "),s("code",[e._v("\\n")]),e._v("。")]),e._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[e._v('SELECT * FROM tbl\nINTO OUTFILE "hdfs:/path/to/result_"\nFORMAT AS CSV\nPROPERTIES\n(\n    "broker.name" = "my_broker",\n    "broker.hadoop.security.authentication" = "kerberos",\n    "broker.kerberos_principal" = "doris@YOUR.COM",\n    "broker.kerberos_keytab" = "/home/doris/my.keytab",\n    "column_separator" = ",",\n    "line_delimiter" = "\\n",\n    "max_file_size" = "100MB"\n);\n')])])]),s("p",[e._v("最终生成文件如如果不大于 100MB，则为："),s("code",[e._v("result_0.csv")]),e._v("。")]),e._v(" "),s("p",[e._v("如果大于 100MB，则可能为 "),s("code",[e._v("result_0.csv, result_1.csv, ...")]),e._v("。")])]),e._v(" "),s("li",[s("p",[e._v("示例2")]),e._v(" "),s("p",[e._v("将简单查询结果导出到文件 "),s("code",[e._v("hdfs:/path/to/result.parquet")]),e._v("。指定导出格式为 PARQUET。使用 "),s("code",[e._v("my_broker")]),e._v(" 并设置 kerberos 认证信息。")]),e._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[e._v('SELECT c1, c2, c3 FROM tbl\nINTO OUTFILE "hdfs:/path/to/result_"\nFORMAT AS PARQUET\nPROPERTIES\n(\n    "broker.name" = "my_broker",\n    "broker.hadoop.security.authentication" = "kerberos",\n    "broker.kerberos_principal" = "doris@YOUR.COM",\n    "broker.kerberos_keytab" = "/home/doris/my.keytab",\n    "schema"="required,int32,c1;required,byte_array,c2;required,byte_array,c2"\n);\n')])])]),s("p",[e._v("查询结果导出到parquet文件需要明确指定"),s("code",[e._v("schema")]),e._v("。")])]),e._v(" "),s("li",[s("p",[e._v("示例3")]),e._v(" "),s("p",[e._v("将 CTE 语句的查询结果导出到文件 "),s("code",[e._v("hdfs:/path/to/result.txt")]),e._v("。默认导出格式为 CSV。使用 "),s("code",[e._v("my_broker")]),e._v(" 并设置 hdfs 高可用信息。使用默认的行列分隔符。")]),e._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[e._v('WITH\nx1 AS\n(SELECT k1, k2 FROM tbl1),\nx2 AS\n(SELECT k3 FROM tbl2)\nSELEC k1 FROM x1 UNION SELECT k3 FROM x2\nINTO OUTFILE "hdfs:/path/to/result_"\nPROPERTIES\n(\n    "broker.name" = "my_broker",\n    "broker.username"="user",\n    "broker.password"="passwd",\n    "broker.dfs.nameservices" = "my_ha",\n    "broker.dfs.ha.namenodes.my_ha" = "my_namenode1, my_namenode2",\n    "broker.dfs.namenode.rpc-address.my_ha.my_namenode1" = "nn1_host:rpc_port",\n    "broker.dfs.namenode.rpc-address.my_ha.my_namenode2" = "nn2_host:rpc_port",\n    "broker.dfs.client.failover.proxy.provider" = "org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider"\n);\n')])])]),s("p",[e._v("最终生成文件如如果不大于 1GB，则为："),s("code",[e._v("result_0.csv")]),e._v("。")]),e._v(" "),s("p",[e._v("如果大于 1GB，则可能为 "),s("code",[e._v("result_0.csv, result_1.csv, ...")]),e._v("。")])]),e._v(" "),s("li",[s("p",[e._v("示例4")]),e._v(" "),s("p",[e._v("将 UNION 语句的查询结果导出到文件 "),s("code",[e._v("bos://bucket/result.txt")]),e._v("。指定导出格式为 PARQUET。使用 "),s("code",[e._v("my_broker")]),e._v(" 并设置 hdfs 高可用信息。PARQUET 格式无需指定列分割符。\n导出完成后，生成一个标识文件。")]),e._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[e._v('SELECT k1 FROM tbl1 UNION SELECT k2 FROM tbl1\nINTO OUTFILE "bos://bucket/result_"\nFORMAT AS PARQUET\nPROPERTIES\n(\n    "broker.name" = "my_broker",\n    "broker.bos_endpoint" = "http://bj.bcebos.com",\n    "broker.bos_accesskey" = "xxxxxxxxxxxxxxxxxxxxxxxxxx",\n    "broker.bos_secret_accesskey" = "yyyyyyyyyyyyyyyyyyyyyyyyyy",\n    "schema"="required,int32,k1;required,byte_array,k2"\n);\n')])])])]),e._v(" "),s("li",[s("p",[e._v("示例5")]),e._v(" "),s("p",[e._v("将 select 语句的查询结果导出到文件 "),s("code",[e._v("cos://${bucket_name}/path/result.txt")]),e._v("。指定导出格式为 csv。\n导出完成后，生成一个标识文件。")]),e._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[e._v('select k1,k2,v1 from tbl1 limit 100000\ninto outfile "s3a://my_bucket/export/my_file_"\nFORMAT AS CSV\nPROPERTIES\n(\n    "broker.name" = "hdfs_broker",\n    "broker.fs.s3a.access.key" = "xxx",\n    "broker.fs.s3a.secret.key" = "xxxx",\n    "broker.fs.s3a.endpoint" = "https://cos.xxxxxx.myqcloud.com/",\n    "column_separator" = ",",\n    "line_delimiter" = "\\n",\n    "max_file_size" = "1024MB",\n    "success_file_name" = "SUCCESS"\n)\n')])])]),s("p",[e._v("最终生成文件如如果不大于 1GB，则为："),s("code",[e._v("my_file_0.csv")]),e._v("。")]),e._v(" "),s("p",[e._v("如果大于 1GB，则可能为 "),s("code",[e._v("my_file_0.csv, result_1.csv, ...")]),e._v("。")]),e._v(" "),s("p",[e._v("在cos上验证")]),e._v(" "),s("ol",[s("li",[e._v("不存在的path会自动创建")]),e._v(" "),s("li",[e._v("access.key/secret.key/endpoint需要和cos的同学确认。尤其是endpoint的值，不需要填写bucket_name。")])])]),e._v(" "),s("li",[s("p",[e._v("示例6")]),e._v(" "),s("p",[e._v("使用 s3 协议导出到 bos，并且并发导出开启。")]),e._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[e._v('set enable_parallel_outfile = true;\nselect k1 from tb1 limit 1000\ninto outfile "s3://my_bucket/export/my_file_"\nformat as csv\nproperties\n(\n    "AWS_ENDPOINT" = "http://s3.bd.bcebos.com",\n    "AWS_ACCESS_KEY" = "xxxx",\n    "AWS_SECRET_KEY" = "xxx",\n    "AWS_REGION" = "bd"\n)\n')])])]),s("p",[e._v("最终生成的文件前缀为 "),s("code",[e._v("my_file_{fragment_instance_id}_")]),e._v("。")])]),e._v(" "),s("li",[s("p",[e._v("示例7")]),e._v(" "),s("p",[e._v("使用 s3 协议导出到 bos，并且并发导出 session 变量开启。")]),e._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[e._v('set enable_parallel_outfile = true;\nselect k1 from tb1 order by k1 limit 1000\ninto outfile "s3://my_bucket/export/my_file_"\nformat as csv\nproperties\n(\n    "AWS_ENDPOINT" = "http://s3.bd.bcebos.com",\n    "AWS_ACCESS_KEY" = "xxxx",\n    "AWS_SECRET_KEY" = "xxx",\n    "AWS_REGION" = "bd"\n)\n')])])]),s("p",[s("strong",[e._v("但由于查询语句带了一个顶层的排序节点，所以这个查询即使开启并发导出的 session 变量，也是无法并发导出的。")])])]),e._v(" "),s("li",[s("p",[e._v("示例7")]),e._v(" "),s("p",[e._v("使用 hdfs 方式导出，将简单查询结果导出到文件 "),s("code",[e._v("hdfs:/path/to/result.txt")]),e._v("。指定导出格式为 CSV。使用并设置 kerberos 认证信息。")]),e._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[e._v('SELECT * FROM tbl\nINTO OUTFILE "hdfs://path/to/result_"\nFORMAT AS CSV\nPROPERTIES\n(\n    "hdfs.fs.defaultFS" = "hdfs://namenode:port",\n    "hdfs.hadoop.security.authentication" = "kerberos",\n    "hdfs.kerberos_principal" = "doris@YOUR.COM",\n    "hdfs.kerberos_keytab" = "/home/doris/my.keytab"\n);\n')])])])])]),e._v(" "),s("h2",{attrs:{id:"返回结果"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#返回结果"}},[e._v("#")]),e._v(" 返回结果")]),e._v(" "),s("p",[e._v("导出命令为同步命令。命令返回，即表示操作结束。同时会返回一行结果来展示导出的执行结果。")]),e._v(" "),s("p",[e._v("如果正常导出并返回，则结果如下：")]),e._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[e._v('mysql> select * from tbl1 limit 10 into outfile "file:///home/work/path/result_";\n+------------+-----------+----------+--------------------------------------------------------------------+\n| FileNumber | TotalRows | FileSize | URL                                                                |\n+------------+-----------+----------+--------------------------------------------------------------------+\n|          1 |         2 |        8 | file:///192.168.1.10/home/work/path/result_{fragment_instance_id}_ |\n+------------+-----------+----------+--------------------------------------------------------------------+\n1 row in set (0.05 sec)\n')])])]),s("ul",[s("li",[e._v("FileNumber：最终生成的文件个数。")]),e._v(" "),s("li",[e._v("TotalRows：结果集行数。")]),e._v(" "),s("li",[e._v("FileSize：导出文件总大小。单位字节。")]),e._v(" "),s("li",[e._v("URL：如果是导出到本地磁盘，则这里显示具体导出到哪个 Compute Node。")])]),e._v(" "),s("p",[e._v("如果进行了并发导出，则会返回多行数据。")]),e._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[e._v("+------------+-----------+----------+--------------------------------------------------------------------+\n| FileNumber | TotalRows | FileSize | URL                                                                |\n+------------+-----------+----------+--------------------------------------------------------------------+\n|          1 |         3 |        7 | file:///192.168.1.10/home/work/path/result_{fragment_instance_id}_ |\n|          1 |         2 |        4 | file:///192.168.1.11/home/work/path/result_{fragment_instance_id}_ |\n+------------+-----------+----------+--------------------------------------------------------------------+\n2 rows in set (2.218 sec)\n")])])]),s("p",[e._v("如果执行错误，则会返回错误信息，如：")]),e._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[e._v("mysql> SELECT * FROM tbl INTO OUTFILE ...\nERROR 1064 (HY000): errCode = 2, detailMessage = Open broker writer failed ...\n")])])]),s("h2",{attrs:{id:"注意事项"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#注意事项"}},[e._v("#")]),e._v(" 注意事项")]),e._v(" "),s("ul",[s("li",[e._v("如果不开启并发导出，查询结果是由单个 BE 节点，单线程导出的。因此导出时间和导出结果集大小正相关。开启并发导出可以降低导出的时间。")]),e._v(" "),s("li",[e._v("导出命令不会检查文件及文件路径是否存在。是否会自动创建路径、或是否会覆盖已存在文件，完全由远端存储系统的语义决定。")]),e._v(" "),s("li",[e._v("如果在导出过程中出现错误，可能会有导出文件残留在远端存储系统上。Doris 不会清理这些文件。需要用户手动清理。")]),e._v(" "),s("li",[e._v("导出命令的超时时间同查询的超时时间。可以通过 "),s("code",[e._v("SET query_timeout=xxx")]),e._v(" 进行设置。")]),e._v(" "),s("li",[e._v("对于结果集为空的查询，依然会产生一个大小为0的文件。")]),e._v(" "),s("li",[e._v("文件切分会保证一行数据完整的存储在单一文件中。因此文件的大小并不严格等于 "),s("code",[e._v("max_file_size")]),e._v("。")]),e._v(" "),s("li",[e._v("对于部分输出为非可见字符的函数，如 BITMAP、HLL 类型，输出为 "),s("code",[e._v("\\N")]),e._v("，即 NULL。")]),e._v(" "),s("li",[e._v("目前部分地理信息函数，如 "),s("code",[e._v("ST_Point")]),e._v(" 的输出类型为 VARCHAR，但实际输出值为经过编码的二进制字符。当前这些函数会输出乱码。对于地理函数，请使用 "),s("code",[e._v("ST_AsText")]),e._v(" 进行输出。")])])])}),[],!1,null,null,null);_.default=a.exports}}]);